{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word sentiment classification\n",
    "\n",
    "* many to one\n",
    "* variable input sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential, Model\n",
    "# 추가된 import\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import glob\n",
    "import csv\n",
    "pd.set_option('display.max_row', 40000)\n",
    "pd.set_option('display.max_column', 10000)\n",
    "%matplotlib inline\n",
    "from os.path import join\n",
    "# tf 2.0부터 keras는 tensorflow의 공식 API\n",
    "from tensorflow.keras import layers, models, optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepairing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list: ['src\\\\3051585.db', 'src\\\\3051589.db', 'src\\\\3051590.db', 'src\\\\3051598.db', 'src\\\\3052026.db', 'src\\\\3052098.db', 'src\\\\3052259.db', 'src\\\\3052260.db', 'src\\\\3052470.db']\n"
     ]
    }
   ],
   "source": [
    "path = \"src/*\" #-------------------------자신의 db파일이 있는 폴더로 수정---------------------------\n",
    "file_list =  glob.glob(path)\n",
    "file_list_db = [file for file in file_list if file.endswith(\".db\")]\n",
    "print (\"file_list: {}\".format(file_list_db))\n",
    "len(file_list_db)\n",
    "# EventCode03이 일어났을 당시의 초당 주행기록 가져오기\n",
    "srcrec_df2 = pd.DataFrame() #연결한 db 결과 저장소\n",
    "for i in range (0,len(file_list_db)):\n",
    "    # event 파일 가져오기\n",
    "    f = open('src/event.csv') #-------------------------자신의 event파일이 있는 폴더로 수정---------------------------\n",
    "    csvReader = csv.reader(f)\n",
    "    #db연결\n",
    "    conn = sqlite3.connect(file_list_db[i])\n",
    "    c = conn. cursor()\n",
    "    # event 테이블 유무 확인 후, 있으면 제거\n",
    "    c.execute('Drop Table If Exists event')\n",
    "    # event 테이블 생성\n",
    "    c.execute(\"create table event(CAR_RECDRV_KEY integer, EVENT_CODE text, EVENT_STDT text, EVENT_ENDT text)\")\n",
    "    # csv 파일 읽어 온 데이터 insert\n",
    "    for row in csvReader:\n",
    "        if row[7] == \"EVENT_CODE\":\n",
    "            continue\n",
    "        sql1 = \"insert into event (CAR_RECDRV_KEY,EVENT_CODE, EVENT_STDT, EVENT_ENDT) values (?,?,?,?)\"\n",
    "        key= int(row[1])\n",
    "        #key 추출\n",
    "        if key != int(file_list_db[i][4:11]): #src에 자신의 db파일이 있어야함, 아니면 인덱스 수정할 것\n",
    "            continue\n",
    "        code = (row[7])\n",
    "        #event 추출\n",
    "        if code[-11:] != \"EventCode03\":\n",
    "            continue\n",
    "        stdt = (row[8])\n",
    "        endt = (row[9])\n",
    "        c.execute(sql1,(key,code,stdt,endt))\n",
    "    #트랜잭션 저장\n",
    "    conn.commit()\n",
    "    #event03 뽐기 query 실행\n",
    "    sql2 = 'SELECT SRCREC.srcValue, SRCREC.realTime, SRCREC.srcSpeed, SRCREC.srcAPS,\\\n",
    "    SRCREC.srcGyroValue, SRCREC.srcRPM, SRCREC.srcTPS, SRCREC.srcMAF, SRCREC.srcEngineLoad, ifnull(EVENT_CODE, \"0\") FROM SRCREC LEFT OUTER JOIN\\\n",
    "    (SELECT EVENT_CODE, EVENT_STDT s, EVENT_ENDT e FROM event\\\n",
    "    ) ON SRCREC.realTime BETWEEN strftime(\"%Y%m%d%H%M%S\",s,\"-6 seconds\")\\\n",
    "     AND strftime(\"%Y%m%d%H%M%S\",s,\"-1 seconds\")'\n",
    "    query =  c.execute(sql2)\n",
    "    cols = [column[0] for column in query.description]\n",
    "    srcrec_df = pd.DataFrame.from_records(data=query.fetchall(), columns=cols)\n",
    "    #db 파일 연결 작업\n",
    "    srcrec_df2 = pd.concat([srcrec_df2, srcrec_df], ignore_index=True)\n",
    "    #db 연결 종료\n",
    "    c.close()\n",
    "    conn.close()\n",
    "    #파일 연결 종료\n",
    "    f.close()\n",
    "srcrec_df = srcrec_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = srcrec_df.apply(pd.to_numeric,errors='coerce')\n",
    "\n",
    "x_data = x_data.iloc[:,2:9]\n",
    "x_data = x_data.values\n",
    "x_data= x_data.reshape(len(x_data),-1)\n",
    "x_data = x_data[:,:,np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "y_data = srcrec_df.values[:,9]\n",
    "y_data = pd.get_dummies(y_data).values\n",
    "y_data = y_data[:,0]\n",
    "\n",
    "where_0 = np.where(y_data ==0)\n",
    "where_1 = np.where(y_data ==1)\n",
    "\n",
    "y_data[where_0] = 1\n",
    "y_data[where_1] = 0\n",
    "y_data = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating model\n",
    "#input_dim = len(char2idx)\n",
    "#output_dim = len(char2idx)\n",
    "#one_hot = np.eye(len(char2idx))\n",
    "num_classes = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.SimpleRNN(units=10, return_sequences=False, input_shape=[7,1]))\n",
    "model.add(layers.Dense(units=num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 142\n",
      "Trainable params: 142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss function\n",
    "def loss_fn(model, x, y):\n",
    "    #print(y.shape)\n",
    "    #print(model(x).shape)\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true= y, y_pred= model(x), from_logits=True))\n",
    "\n",
    "#create optimizer\n",
    "lr=.01\n",
    "epochs=100\n",
    "batch_size = 2\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 7, 1), (None, 2)), types: (tf.float64, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "# generating data pipeline\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size = 4)\n",
    "tr_dataset = tr_dataset.batch(batch_size = batch_size)\n",
    "\n",
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   5, tr_loss : 0.133\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "tr_loss_hist =[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    for x_mb, y_mb in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, x=x_mb, y=y_mb)\n",
    "        grads = tape.gradient(target=tr_loss, sources=model.variables)\n",
    "        opt.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "        avg_tr_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_tr_loss /= tr_step\n",
    "        tr_loss_hist.append(avg_tr_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 ==0:        \n",
    "        print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x_data)\n",
    "yhat = np.argmax(yhat, axis=-1)\n",
    "print('acc : {:.2%}'.format(np.mean(yhat == y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
